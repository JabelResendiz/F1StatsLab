{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis para la prediccion de Tiempo de Vuelta usando Regresion Lineal Multiple\n",
    "\n",
    "\n",
    "`Melbourne` (Gran Premio de Australia) es un circuito urbano ubicado en Albert Park, `Melbourne`. Con una longitud de 5303 metros, se caracteriza por ser una pista mixta que combina secciones de alta velocidad con curvas t√©cnicas. Aunque es un circuito que se corre en sentido horario, las zonas m√°s dif√≠ciles son aquellas con m√∫ltiples cambios de direcci√≥n, lo que exige un alto nivel de control y precisi√≥n.\n",
    "\n",
    "El clima en `Melbourne` es impredecible, con cambios repentinos de temperatura y posibles lluvias que complican las estrategias de los equipos. Las curvas de alta velocidad y las rectas relativamente cortas hacen que las paradas en boxes sean cruciales para los pilotos. Por lo tanto, es esencial tener un buen manejo de los neum√°ticos, especialmente en las zonas donde el asfalto es m√°s abrasivo. El desempe√±o en la frenada y las aceleraciones de las curvas 1 y 3 son clave para conseguir tiempos r√°pidos.\n",
    "\n",
    "![Melbourne](../img/melbourne.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos entonces un estudio de un modelo de regresi√≥n lineal m√∫ltiple con variable dependiente `FinalRaceTime` en el circuito de Melbourne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MaxSpeed', 'Age', 'FinalPosition', 'Points', 'Overtakes', 'TyreWear', 'WeatherCondition_Mixed', 'TrackGrip_Low']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "def load_data(filepath, circuit_name):\n",
    "    data = pd.read_csv(filepath)\n",
    "    circuit_data = data[data['Circuit'] == circuit_name].copy()\n",
    "    if circuit_data.empty:\n",
    "        raise ValueError(f\"No data found for circuit {circuit_name}\")\n",
    "    return circuit_data\n",
    "\n",
    "def select_features(data, target, initial_features):\n",
    "    features = [f for f in initial_features if f in data.columns]\n",
    "    if target not in data.columns:\n",
    "        raise ValueError(f\"Target column '{target}' not found in the dataset\")\n",
    "    return features\n",
    "\n",
    "def preprocess_data(data, features, target):\n",
    "    for feature in features:\n",
    "        data[feature] = pd.to_numeric(data[feature], errors='coerce')\n",
    "    data = data.dropna(subset=features + [target])\n",
    "    return data\n",
    "\n",
    "def train_model(X, y):\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    # Create and train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Print the equation of the hyperplane\n",
    "    coefficients = model.coef_\n",
    "    intercept = model.intercept_\n",
    "\n",
    "    #print(\"\\nEquation of the hyperplane:\")\n",
    "    equation = f\"FinalRiceTime = {intercept:.2f}\"\n",
    "    for feature, coef in zip(features, coefficients):\n",
    "        equation += f\" + ({coef:.2f} * {feature})\"\n",
    "    #print(equation)\n",
    "\n",
    "    # Calculate R-squared\n",
    "    r_squared = model.score(X_test_scaled, y_test)\n",
    "    \n",
    "   \n",
    "# Agregar constante para la intersecci√≥n\n",
    "    X_train_const = sm.add_constant(X_train_scaled)\n",
    "\n",
    "\n",
    "# Ajustar el modelo de regresi√≥n con statsmodels\n",
    "    model_sm = sm.OLS(y_train, X_train_const).fit()\n",
    "\n",
    "# Mostrar los p-values de cada coeficiente\n",
    "    #print(model_sm.summary())\n",
    "\n",
    "# Create a DataFrame for coefficients and p-values\n",
    "    summary_df = pd.DataFrame({\n",
    "    'Feature': model_sm.params.index,\n",
    "    'Coefficient': model_sm.params.values,\n",
    "    'P-value': model_sm.pvalues.values\n",
    "    })\n",
    "\n",
    "# Filter to keep only p-values greater than 0.05\n",
    "    summary_df = summary_df[summary_df['P-value'] > 0.05]\n",
    "\n",
    "# Sort the DataFrame by p-value in descending order\n",
    "    summary_df = summary_df.sort_values(by='P-value', ascending=False)\n",
    "\n",
    "\n",
    "# Print the sorted DataFrame\n",
    "    # print(\"\\nSorted Coefficients and P-values (from highest to lowest p-value):\")\n",
    "    # print(summary_df)\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "\n",
    "# Ejecutar el pipeline\n",
    "data = load_data(\"formula1_interlagos_df_final.csv\", \"Melbourne\")\n",
    "features = ['MaxSpeed', 'DriverSkill', 'Age', 'PitStopTime', 'ReactionTime',\n",
    "                    'FinalPosition', 'Experience', 'DNF', 'Points', 'Overtakes', 'TyreWear',\n",
    "                    'CarPerformance', 'TrackFamiliarity', 'FuelConsumption', 'DownforceLevel',\n",
    "                    'TrackTemperature', 'WeatherCondition_Mixed', 'WeatherCondition_Wet',\n",
    "                    'TyreCompound_Medium', 'TyreCompound_Soft', 'TrackGrip_Low', 'TrackGrip_Medium']\n",
    "\n",
    "target = 'FinalRaceTime'\n",
    "features = select_features(data, target, features)\n",
    "data = preprocess_data(data, features, target)\n",
    "X, y = data[features], data[target]\n",
    "\n",
    "summary_df = train_model(X, y)\n",
    "\n",
    "while len(summary_df)>0:\n",
    "    top_feature = str(summary_df.iloc[0]['Feature']) # Primer variable (con mayor p-valor)\n",
    "    \n",
    "    #print(top_feature[1:])\n",
    "    \n",
    "    del features[int(top_feature[1:])-1]\n",
    "    #print(f\"Variable {top_feature} eliminada de 'features'.\")\n",
    "    #print(f\"Lista de variables restantes: {features}\")\n",
    "    \n",
    "    features = select_features(data, target, features)\n",
    "    data = preprocess_data(data, features, target)\n",
    "    X, y = data[features], data[target]\n",
    "    summary_df = train_model(X, y)\n",
    "    \n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqu√≠ hemos podido ver que aplicamos un m√©todo de selecci√≥n de caracter√≠sticas basado en `backward elimination`. En cada iteraci√≥n, eliminamos la variable con el mayor p-valor (es decir, la menos significativa), y repetimos el proceso hasta que todas las variables restantes tienen un p-valor menor a 0.05, lo que sugiere que son estad√≠sticamente significativas para predecir el tiempo final de la carrera. Finalmente, nos hemos quedado con un conjunto reducido de variables independientes que tienen una mayor relaci√≥n con el objetivo de la predicci√≥n, FinalRaceTime, lo que mejora la precisi√≥n del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          FinalRaceTime   R-squared:                       0.911\n",
      "Model:                            OLS   Adj. R-squared:                  0.880\n",
      "Method:                 Least Squares   F-statistic:                     29.53\n",
      "Date:                Tue, 04 Mar 2025   Prob (F-statistic):           2.52e-10\n",
      "Time:                        16:20:10   Log-Likelihood:                -80.985\n",
      "No. Observations:                  32   AIC:                             180.0\n",
      "Df Residuals:                      23   BIC:                             193.2\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        267.6977      0.634    422.321      0.000     266.386     269.009\n",
      "x1            -3.8545      0.669     -5.757      0.000      -5.239      -2.470\n",
      "x2            -2.5907      0.703     -3.684      0.001      -4.046      -1.136\n",
      "x3            13.9534      2.107      6.621      0.000       9.594      18.313\n",
      "x4             8.8816      2.190      4.055      0.000       4.351      13.412\n",
      "x5            -2.7761      0.960     -2.890      0.008      -4.763      -0.789\n",
      "x6            -5.2838      0.688     -7.678      0.000      -6.707      -3.860\n",
      "x7             4.2413      0.864      4.908      0.000       2.454       6.029\n",
      "x8             4.4995      0.701      6.416      0.000       3.049       5.950\n",
      "==============================================================================\n",
      "Omnibus:                        0.303   Durbin-Watson:                   2.456\n",
      "Prob(Omnibus):                  0.859   Jarque-Bera (JB):                0.194\n",
      "Skew:                           0.176   Prob(JB):                        0.907\n",
      "Kurtosis:                       2.853   Cond. No.                         7.53\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_model2(X, y):\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    # Create and train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Print the equation of the hyperplane\n",
    "    coefficients = model.coef_\n",
    "    intercept = model.intercept_\n",
    "\n",
    "    #print(\"\\nEquation of the hyperplane:\")\n",
    "    equation = f\"FinalRiceTime = {intercept:.2f}\"\n",
    "    for feature, coef in zip(features, coefficients):\n",
    "        equation += f\" + ({coef:.2f} * {feature})\"\n",
    "    #print(equation)\n",
    "\n",
    "    # Calculate R-squared\n",
    "    r_squared = model.score(X_test_scaled, y_test)\n",
    "    \n",
    "   \n",
    "# Agregar constante para la intersecci√≥n\n",
    "    X_train_const = sm.add_constant(X_train_scaled)\n",
    "\n",
    "\n",
    "# Ajustar el modelo de regresi√≥n con statsmodels\n",
    "    model_sm = sm.OLS(y_train, X_train_const).fit()\n",
    "\n",
    "# Mostrar los p-values de cada coeficiente\n",
    "    print(model_sm.summary())\n",
    "\n",
    "\n",
    "X, y = data[features], data[target]\n",
    "summary_df = train_model2(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An√°lisis del Modelo de Regresi√≥n OLS\n",
    "\n",
    "El modelo de regresi√≥n lineal ajustado tiene como variable dependiente FinalRaceTime y utiliza las siguientes variables predictoras:\n",
    "\n",
    "- MaxSpeed\n",
    "- Age\n",
    "- FinalPosition\n",
    "- Points\n",
    "- Overtakes\n",
    "- TyreWear\n",
    "- WeatherCondition_Mixed\n",
    "- TrackGrip_Low\n",
    "\n",
    "#### Evaluaci√≥n General del Modelo\n",
    "\n",
    "- $R^2$ = 0.911 -> Indica que el 91.1% de la variabilidad en el `FinalRaceTime` puede ser explicada por las variables predictoras del modelo.\n",
    "- $R^2$ ajustado = 0.880 -> Penaliza el $R^2$  por el n√∫mero de predictores. Aunque es menor, sigue siendo alto, lo que sugiere un buen ajuste del modelo.\n",
    "\n",
    "El modelo explica una gran parte de la variabilidad en los tiempos de carrera. Un\n",
    "\n",
    "$R^2$ mayor a 0.8 sugiere un buen modelo en t√©rminos de ajuste.\n",
    "\n",
    "#### Significancia Global del Modelo\n",
    "\n",
    "``Prueba F-Statistic`` : prueba estad√≠stica utilizada para evaluar la **significancia global** del modelo de regresi√≥n. Responde a las pregunta:\n",
    "\n",
    "> ¬øAl menos una de las variables predictoras tiene un efecto significativo sobre la variable dependiente?\n",
    "\n",
    "- F = 29.53 con p-value = 2.52e-10\n",
    "- Esto indica que al menos una de las variables predictoras tiene un impacto significativo en FinalRaceTime.\n",
    "\n",
    "#### An√°lisis de los Coeficientes y Significancia Individual\n",
    "\n",
    "Cada coeficiente indica cu√°nto cambia FinalRaceTime cuando la variable predictora aumenta en una unidad, manteniendo las dem√°s constantes.\n",
    "\n",
    "| Variable                    | Coeficiente | p-valor | Interpretacion                                                                                                                 |\n",
    "| --------------------------- | ----------- | ------- | ------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| Intercept (Constante)       | 267.70      | 0.000   | Tiempo base sin efectos de las variables predictoras                                                                           |\n",
    "| MaxSpeed (x1)               | -3.85       | 0.000   | A mayor velocidad m√°xima, menor tiempo de carrera                                                                             |\n",
    "| Age (x2)                    | -2.59       | 0.001   | A mayor edad, menor tiempo de carrera (pilotos m√°s experimentados pueden ser m√°s r√°pidos)                                   |\n",
    "| FinalPosition (x3)          | 13.95       | 0.000   | A medida que la posici√≥n final aumenta, el tiempo de carrera es mayor (pilotos m√°s lentos terminan en posiciones m√°s bajas) |\n",
    "| Points (x4)                 | 8.88        | 0.000   |                                                                                                                                |\n",
    "| Overtakes (x5)              | -2.77       | 0.008   | M√°s adelantamientos reducen el tiempo de carrera (pilotos agresivos ganan tiempo)                                             |\n",
    "| TyreWear (x6)               | -5.28       | 0.000   | M√°s desgaste de neum√°ticos, menor tiempo (posiblemente estrategia agresiva con neum√°ticos blandos)                          |\n",
    "| WeatherCondition_Mixed (x7) | 4.24        | 0.000   | Condiciones clim√°ticas mixtas aumentan el tiempo de carrera                                                                   |\n",
    "| TrackGrip_Low (x8)          | 4.50        | 0.000   | Bajo agarre de pista aumenta el tiempo de carrera                                                                              |\n",
    "\n",
    "\n",
    "**Conclusi√≥n:**\n",
    "\n",
    "- Todos los **p-valores** son  **< 0.05** , lo que significa que  **todas las variables son significativas** .\n",
    "- Los signos de los coeficientes tienen  **interpretaciones l√≥gicas** , lo que refuerza la validez del modelo.\n",
    "- El modelo parece ser  **robusto y bien especificado** .\n",
    "\n",
    "\n",
    "## **Conclusi√≥n Final**\n",
    "\n",
    "‚úÖ  **El modelo es s√≥lido** , con un  **buen ajuste (R2=91.1R^2 = 91.1%**R**2**=**91.1)** , **predictores significativos** y  **una interpretaci√≥n l√≥gica de los coeficientes** .\n",
    "\n",
    "üìå  **Pr√≥ximos Pasos** :\n",
    "\n",
    "Antes de concluir la investigaci√≥n, es importante validar los **supuestos del modelo** (normalidad, homoscedasticidad, independencia y multicolinealidad). Esto asegurar√° que los resultados sean confiables. üöÄ\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
