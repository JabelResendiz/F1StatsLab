{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis para la prediccion de Tiempo de Vuelta usando Regresion Lineal Multiple\n",
    "\n",
    "\n",
    "`Melbourne` (Gran Premio de Australia) es un circuito urbano ubicado en Albert Park, `Melbourne`. Con una longitud de 5303 metros, se caracteriza por ser una pista mixta que combina secciones de alta velocidad con curvas técnicas. Aunque es un circuito que se corre en sentido horario, las zonas más difíciles son aquellas con múltiples cambios de dirección, lo que exige un alto nivel de control y precisión.\n",
    "\n",
    "El clima en `Melbourne` es impredecible, con cambios repentinos de temperatura y posibles lluvias que complican las estrategias de los equipos. Las curvas de alta velocidad y las rectas relativamente cortas hacen que las paradas en boxes sean cruciales para los pilotos. Por lo tanto, es esencial tener un buen manejo de los neumáticos, especialmente en las zonas donde el asfalto es más abrasivo. El desempeño en la frenada y las aceleraciones de las curvas 1 y 3 son clave para conseguir tiempos rápidos.\n",
    "\n",
    "![Melbourne](../img/melbourne.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos entonces un estudio de un modelo de regresión lineal múltiple con variable dependiente `FinalRaceTime` en el circuito de Melbourne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MaxSpeed', 'Age', 'FinalPosition', 'Points', 'Overtakes', 'TyreWear', 'WeatherCondition_Mixed', 'TrackGrip_Low']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "def load_data(filepath, circuit_name):\n",
    "    data = pd.read_csv(filepath)\n",
    "    circuit_data = data[data['Circuit'] == circuit_name].copy()\n",
    "    if circuit_data.empty:\n",
    "        raise ValueError(f\"No data found for circuit {circuit_name}\")\n",
    "    return circuit_data\n",
    "\n",
    "def select_features(data, target, initial_features):\n",
    "    features = [f for f in initial_features if f in data.columns]\n",
    "    if target not in data.columns:\n",
    "        raise ValueError(f\"Target column '{target}' not found in the dataset\")\n",
    "    return features\n",
    "\n",
    "def preprocess_data(data, features, target):\n",
    "    for feature in features:\n",
    "        data[feature] = pd.to_numeric(data[feature], errors='coerce')\n",
    "    data = data.dropna(subset=features + [target])\n",
    "    return data\n",
    "\n",
    "def train_model(X, y):\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    # Create and train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Print the equation of the hyperplane\n",
    "    coefficients = model.coef_\n",
    "    intercept = model.intercept_\n",
    "\n",
    "    #print(\"\\nEquation of the hyperplane:\")\n",
    "    equation = f\"FinalRiceTime = {intercept:.2f}\"\n",
    "    for feature, coef in zip(features, coefficients):\n",
    "        equation += f\" + ({coef:.2f} * {feature})\"\n",
    "    #print(equation)\n",
    "\n",
    "    # Calculate R-squared\n",
    "    r_squared = model.score(X_test_scaled, y_test)\n",
    "    \n",
    "   \n",
    "# Agregar constante para la intersección\n",
    "    X_train_const = sm.add_constant(X_train_scaled)\n",
    "\n",
    "\n",
    "# Ajustar el modelo de regresión con statsmodels\n",
    "    model_sm = sm.OLS(y_train, X_train_const).fit()\n",
    "\n",
    "# Mostrar los p-values de cada coeficiente\n",
    "    #print(model_sm.summary())\n",
    "\n",
    "# Create a DataFrame for coefficients and p-values\n",
    "    summary_df = pd.DataFrame({\n",
    "    'Feature': model_sm.params.index,\n",
    "    'Coefficient': model_sm.params.values,\n",
    "    'P-value': model_sm.pvalues.values\n",
    "    })\n",
    "\n",
    "# Filter to keep only p-values greater than 0.05\n",
    "    summary_df = summary_df[summary_df['P-value'] > 0.05]\n",
    "\n",
    "# Sort the DataFrame by p-value in descending order\n",
    "    summary_df = summary_df.sort_values(by='P-value', ascending=False)\n",
    "\n",
    "\n",
    "# Print the sorted DataFrame\n",
    "    # print(\"\\nSorted Coefficients and P-values (from highest to lowest p-value):\")\n",
    "    # print(summary_df)\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "\n",
    "# Ejecutar el pipeline\n",
    "data = load_data(\"formula1_interlagos_df_final.csv\", \"Melbourne\")\n",
    "features = ['MaxSpeed', 'DriverSkill', 'Age', 'PitStopTime', 'ReactionTime',\n",
    "                    'FinalPosition', 'Experience', 'DNF', 'Points', 'Overtakes', 'TyreWear',\n",
    "                    'CarPerformance', 'TrackFamiliarity', 'FuelConsumption', 'DownforceLevel',\n",
    "                    'TrackTemperature', 'WeatherCondition_Mixed', 'WeatherCondition_Wet',\n",
    "                    'TyreCompound_Medium', 'TyreCompound_Soft', 'TrackGrip_Low', 'TrackGrip_Medium']\n",
    "\n",
    "target = 'FinalRaceTime'\n",
    "features = select_features(data, target, features)\n",
    "data = preprocess_data(data, features, target)\n",
    "X, y = data[features], data[target]\n",
    "\n",
    "summary_df = train_model(X, y)\n",
    "\n",
    "while len(summary_df)>0:\n",
    "    top_feature = str(summary_df.iloc[0]['Feature']) # Primer variable (con mayor p-valor)\n",
    "    \n",
    "    #print(top_feature[1:])\n",
    "    \n",
    "    del features[int(top_feature[1:])-1]\n",
    "    #print(f\"Variable {top_feature} eliminada de 'features'.\")\n",
    "    #print(f\"Lista de variables restantes: {features}\")\n",
    "    \n",
    "    features = select_features(data, target, features)\n",
    "    data = preprocess_data(data, features, target)\n",
    "    X, y = data[features], data[target]\n",
    "    summary_df = train_model(X, y)\n",
    "    \n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí hemos podido ver que aplicamos un método de selección de características basado en `backward elimination`. En cada iteración, eliminamos la variable con el mayor p-valor (es decir, la menos significativa), y repetimos el proceso hasta que todas las variables restantes tienen un p-valor menor a 0.05, lo que sugiere que son estadísticamente significativas para predecir el tiempo final de la carrera. Finalmente, nos hemos quedado con un conjunto reducido de variables independientes que tienen una mayor relación con el objetivo de la predicción, FinalRaceTime, lo que mejora la precisión del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          FinalRaceTime   R-squared:                       0.911\n",
      "Model:                            OLS   Adj. R-squared:                  0.880\n",
      "Method:                 Least Squares   F-statistic:                     29.53\n",
      "Date:                Tue, 04 Mar 2025   Prob (F-statistic):           2.52e-10\n",
      "Time:                        16:20:10   Log-Likelihood:                -80.985\n",
      "No. Observations:                  32   AIC:                             180.0\n",
      "Df Residuals:                      23   BIC:                             193.2\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        267.6977      0.634    422.321      0.000     266.386     269.009\n",
      "x1            -3.8545      0.669     -5.757      0.000      -5.239      -2.470\n",
      "x2            -2.5907      0.703     -3.684      0.001      -4.046      -1.136\n",
      "x3            13.9534      2.107      6.621      0.000       9.594      18.313\n",
      "x4             8.8816      2.190      4.055      0.000       4.351      13.412\n",
      "x5            -2.7761      0.960     -2.890      0.008      -4.763      -0.789\n",
      "x6            -5.2838      0.688     -7.678      0.000      -6.707      -3.860\n",
      "x7             4.2413      0.864      4.908      0.000       2.454       6.029\n",
      "x8             4.4995      0.701      6.416      0.000       3.049       5.950\n",
      "==============================================================================\n",
      "Omnibus:                        0.303   Durbin-Watson:                   2.456\n",
      "Prob(Omnibus):                  0.859   Jarque-Bera (JB):                0.194\n",
      "Skew:                           0.176   Prob(JB):                        0.907\n",
      "Kurtosis:                       2.853   Cond. No.                         7.53\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_model2(X, y):\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    # Create and train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Print the equation of the hyperplane\n",
    "    coefficients = model.coef_\n",
    "    intercept = model.intercept_\n",
    "\n",
    "    #print(\"\\nEquation of the hyperplane:\")\n",
    "    equation = f\"FinalRiceTime = {intercept:.2f}\"\n",
    "    for feature, coef in zip(features, coefficients):\n",
    "        equation += f\" + ({coef:.2f} * {feature})\"\n",
    "    #print(equation)\n",
    "\n",
    "    # Calculate R-squared\n",
    "    r_squared = model.score(X_test_scaled, y_test)\n",
    "    \n",
    "   \n",
    "# Agregar constante para la intersección\n",
    "    X_train_const = sm.add_constant(X_train_scaled)\n",
    "\n",
    "\n",
    "# Ajustar el modelo de regresión con statsmodels\n",
    "    model_sm = sm.OLS(y_train, X_train_const).fit()\n",
    "\n",
    "# Mostrar los p-values de cada coeficiente\n",
    "    print(model_sm.summary())\n",
    "\n",
    "\n",
    "X, y = data[features], data[target]\n",
    "summary_df = train_model2(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis del Modelo de Regresión OLS\n",
    "\n",
    "El modelo de regresión lineal ajustado tiene como variable dependiente FinalRaceTime y utiliza las siguientes variables predictoras:\n",
    "\n",
    "- MaxSpeed\n",
    "- Age\n",
    "- FinalPosition\n",
    "- Points\n",
    "- Overtakes\n",
    "- TyreWear\n",
    "- WeatherCondition_Mixed\n",
    "- TrackGrip_Low\n",
    "\n",
    "#### Evaluación General del Modelo\n",
    "\n",
    "- $R^2$ = 0.911 -> Indica que el 91.1% de la variabilidad en el `FinalRaceTime` puede ser explicada por las variables predictoras del modelo.\n",
    "- $R^2$ ajustado = 0.880 -> Penaliza el $R^2$  por el número de predictores. Aunque es menor, sigue siendo alto, lo que sugiere un buen ajuste del modelo.\n",
    "\n",
    "El modelo explica una gran parte de la variabilidad en los tiempos de carrera. Un\n",
    "\n",
    "$R^2$ mayor a 0.8 sugiere un buen modelo en términos de ajuste.\n",
    "\n",
    "#### Significancia Global del Modelo\n",
    "\n",
    "``Prueba F-Statistic`` : prueba estadística utilizada para evaluar la **significancia global** del modelo de regresión. Responde a las pregunta:\n",
    "\n",
    "> ¿Al menos una de las variables predictoras tiene un efecto significativo sobre la variable dependiente?\n",
    "\n",
    "- F = 29.53 con p-value = 2.52e-10\n",
    "- Esto indica que al menos una de las variables predictoras tiene un impacto significativo en FinalRaceTime.\n",
    "\n",
    "#### Análisis de los Coeficientes y Significancia Individual\n",
    "\n",
    "Cada coeficiente indica cuánto cambia FinalRaceTime cuando la variable predictora aumenta en una unidad, manteniendo las demás constantes.\n",
    "\n",
    "| Variable                    | Coeficiente | p-valor | Interpretacion                                                                                                                 |\n",
    "| --------------------------- | ----------- | ------- | ------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| Intercept (Constante)       | 267.70      | 0.000   | Tiempo base sin efectos de las variables predictoras                                                                           |\n",
    "| MaxSpeed (x1)               | -3.85       | 0.000   | A mayor velocidad máxima, menor tiempo de carrera                                                                             |\n",
    "| Age (x2)                    | -2.59       | 0.001   | A mayor edad, menor tiempo de carrera (pilotos más experimentados pueden ser más rápidos)                                   |\n",
    "| FinalPosition (x3)          | 13.95       | 0.000   | A medida que la posición final aumenta, el tiempo de carrera es mayor (pilotos más lentos terminan en posiciones más bajas) |\n",
    "| Points (x4)                 | 8.88        | 0.000   |                                                                                                                                |\n",
    "| Overtakes (x5)              | -2.77       | 0.008   | Más adelantamientos reducen el tiempo de carrera (pilotos agresivos ganan tiempo)                                             |\n",
    "| TyreWear (x6)               | -5.28       | 0.000   | Más desgaste de neumáticos, menor tiempo (posiblemente estrategia agresiva con neumáticos blandos)                          |\n",
    "| WeatherCondition_Mixed (x7) | 4.24        | 0.000   | Condiciones climáticas mixtas aumentan el tiempo de carrera                                                                   |\n",
    "| TrackGrip_Low (x8)          | 4.50        | 0.000   | Bajo agarre de pista aumenta el tiempo de carrera                                                                              |\n",
    "\n",
    "\n",
    "**Conclusión:**\n",
    "\n",
    "- Todos los **p-valores** son  **< 0.05** , lo que significa que  **todas las variables son significativas** .\n",
    "- Los signos de los coeficientes tienen  **interpretaciones lógicas** , lo que refuerza la validez del modelo.\n",
    "- El modelo parece ser  **robusto y bien especificado** .\n",
    "\n",
    "\n",
    "## **Conclusión Final**\n",
    "\n",
    "✅  **El modelo es sólido** , con un  **buen ajuste (R2=91.1R^2 = 91.1%**R**2**=**91.1)** , **predictores significativos** y  **una interpretación lógica de los coeficientes** .\n",
    "\n",
    "📌  **Próximos Pasos** :\n",
    "\n",
    "Antes de concluir la investigación, es importante validar los **supuestos del modelo** (normalidad, homoscedasticidad, independencia y multicolinealidad). Esto asegurará que los resultados sean confiables. 🚀\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
